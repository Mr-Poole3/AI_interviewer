"""
è¯­éŸ³èŠå¤©é›†æˆæ¨¡å—

å°†Geminiè¯­éŸ³èŠå¤©åŠŸèƒ½é›†æˆåˆ°ç°æœ‰çš„LLMé¢è¯•å®˜ç³»ç»Ÿä¸­
æ”¯æŒè¯­éŸ³é¢è¯•ã€æ–‡æœ¬é¢è¯•çš„æ— ç¼åˆ‡æ¢
"""
import logging
from typing import Optional, Dict, Any
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
import json

# å¯¼å…¥ç°æœ‰æ¨¡å—
from .voice_chat import VoiceChatStream, create_voice_chat_stream
from .app import app as main_app

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class VoiceInterviewIntegration:
    """
    è¯­éŸ³é¢è¯•é›†æˆå™¨
    
    å°†è¯­éŸ³èŠå¤©åŠŸèƒ½é›†æˆåˆ°ç°æœ‰é¢è¯•ç³»ç»Ÿä¸­
    """
    
    def __init__(self, main_app: FastAPI):
        """
        åˆå§‹åŒ–è¯­éŸ³é¢è¯•é›†æˆå™¨
        
        Args:
            main_app: ä¸»FastAPIåº”ç”¨å®ä¾‹
        """
        self.main_app = main_app
        self.voice_stream = None
        self.active_sessions: Dict[str, Any] = {}
        
        logger.info("è¯­éŸ³é¢è¯•é›†æˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def setup_voice_chat(self, api_key: Optional[str] = None, model_name: Optional[str] = None):
        """
        è®¾ç½®è¯­éŸ³èŠå¤©åŠŸèƒ½
        
        Args:
            api_key: Google AI APIå¯†é’¥
            model_name: Geminiæ¨¡å‹åç§°
        """
        try:
            self.voice_stream = create_voice_chat_stream(api_key, model_name)
            logger.info("è¯­éŸ³èŠå¤©åŠŸèƒ½è®¾ç½®å®Œæˆ")
        except Exception as e:
            logger.error(f"è¯­éŸ³èŠå¤©åŠŸèƒ½è®¾ç½®å¤±è´¥: {e}")
            raise
    
    def mount_voice_endpoints(self):
        """
        æŒ‚è½½è¯­éŸ³ç›¸å…³çš„APIç«¯ç‚¹åˆ°ä¸»åº”ç”¨
        """
        if not self.voice_stream:
            raise ValueError("è¯­éŸ³èŠå¤©åŠŸèƒ½æœªè®¾ç½®ï¼Œè¯·å…ˆè°ƒç”¨setup_voice_chat()")
        
        # æŒ‚è½½è¯­éŸ³æµåˆ°ä¸»åº”ç”¨
        self.voice_stream.mount_to_app(self.main_app)
        
        # æ·»åŠ è¯­éŸ³é¢è¯•é¡µé¢è·¯ç”±
        @self.main_app.get("/voice-interview")
        async def voice_interview_page():
            """è¯­éŸ³é¢è¯•é¡µé¢"""
            return HTMLResponse(content=self._get_voice_interview_html())
        
        # æ·»åŠ è¯­éŸ³é¢è¯•WebSocketç«¯ç‚¹
        @self.main_app.websocket("/ws/voice-interview")
        async def voice_interview_websocket(websocket: WebSocket):
            """è¯­éŸ³é¢è¯•WebSocketè¿æ¥"""
            await self._handle_voice_interview_websocket(websocket)
        
        logger.info("è¯­éŸ³ç«¯ç‚¹æŒ‚è½½å®Œæˆ")
    
    async def _handle_voice_interview_websocket(self, websocket: WebSocket):
        """
        å¤„ç†è¯­éŸ³é¢è¯•WebSocketè¿æ¥
        
        Args:
            websocket: WebSocketè¿æ¥
        """
        await websocket.accept()
        session_id = id(websocket)
        self.active_sessions[session_id] = {
            "websocket": websocket,
            "type": "voice",
            "status": "connected"
        }
        
        try:
            logger.info(f"è¯­éŸ³é¢è¯•ä¼šè¯å¼€å§‹: {session_id}")
            
            while True:
                # æ¥æ”¶æ¶ˆæ¯
                data = await websocket.receive_text()
                message_data = json.loads(data)
                
                # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
                if message_data.get("type") == "voice_data":
                    # å¤„ç†è¯­éŸ³æ•°æ®
                    await self._process_voice_data(websocket, message_data)
                elif message_data.get("type") == "control":
                    # å¤„ç†æ§åˆ¶æ¶ˆæ¯
                    await self._process_control_message(websocket, message_data)
                
        except WebSocketDisconnect:
            logger.info(f"è¯­éŸ³é¢è¯•ä¼šè¯æ–­å¼€: {session_id}")
        except Exception as e:
            logger.error(f"è¯­éŸ³é¢è¯•ä¼šè¯é”™è¯¯: {e}")
        finally:
            # æ¸…ç†ä¼šè¯
            if session_id in self.active_sessions:
                del self.active_sessions[session_id]
    
    async def _process_voice_data(self, websocket: WebSocket, message_data: Dict[str, Any]):
        """
        å¤„ç†è¯­éŸ³æ•°æ®
        
        Args:
            websocket: WebSocketè¿æ¥
            message_data: æ¶ˆæ¯æ•°æ®
        """
        try:
            # è¿™é‡Œå¯ä»¥é›†æˆè¯­éŸ³å¤„ç†é€»è¾‘
            # ä¾‹å¦‚ï¼šè¯­éŸ³è½¬æ–‡å­—ã€AIå›å¤ç”Ÿæˆã€æ–‡å­—è½¬è¯­éŸ³ç­‰
            
            # å‘é€å¤„ç†çŠ¶æ€
            await websocket.send_text(json.dumps({
                "type": "status",
                "message": "æ­£åœ¨å¤„ç†è¯­éŸ³..."
            }))
            
            # æ¨¡æ‹Ÿå¤„ç†ç»“æœ
            response = {
                "type": "voice_response",
                "transcription": "ç”¨æˆ·è¯­éŸ³è½¬å½•ç»“æœ",
                "ai_response": "AIå›å¤å†…å®¹",
                "audio_data": "base64ç¼–ç çš„éŸ³é¢‘æ•°æ®"
            }
            
            await websocket.send_text(json.dumps(response))
            
        except Exception as e:
            logger.error(f"è¯­éŸ³æ•°æ®å¤„ç†å¤±è´¥: {e}")
            await websocket.send_text(json.dumps({
                "type": "error",
                "message": "è¯­éŸ³å¤„ç†å¤±è´¥"
            }))
    
    async def _process_control_message(self, websocket: WebSocket, message_data: Dict[str, Any]):
        """
        å¤„ç†æ§åˆ¶æ¶ˆæ¯
        
        Args:
            websocket: WebSocketè¿æ¥
            message_data: æ¶ˆæ¯æ•°æ®
        """
        try:
            command = message_data.get("command")
            
            if command == "start_recording":
                await websocket.send_text(json.dumps({
                    "type": "status",
                    "message": "å¼€å§‹å½•éŸ³"
                }))
            elif command == "stop_recording":
                await websocket.send_text(json.dumps({
                    "type": "status",
                    "message": "åœæ­¢å½•éŸ³"
                }))
            elif command == "switch_to_text":
                await websocket.send_text(json.dumps({
                    "type": "mode_switch",
                    "new_mode": "text",
                    "message": "å·²åˆ‡æ¢åˆ°æ–‡æœ¬æ¨¡å¼"
                }))
            
        except Exception as e:
            logger.error(f"æ§åˆ¶æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
    
    def _get_voice_interview_html(self) -> str:
        """
        è·å–è¯­éŸ³é¢è¯•é¡µé¢HTML
        
        Returns:
            HTMLé¡µé¢å†…å®¹
        """
        return """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è¯­éŸ³é¢è¯• - AIé¢è¯•å®˜</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        
        .container {
            max-width: 800px;
            margin: 0 auto;
            background: white;
            border-radius: 16px;
            padding: 30px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        }
        
        .header {
            text-align: center;
            margin-bottom: 30px;
        }
        
        .header h1 {
            color: #2c3e50;
            margin-bottom: 10px;
        }
        
        .voice-controls {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin: 30px 0;
        }
        
        .voice-btn {
            padding: 15px 30px;
            border: none;
            border-radius: 50px;
            font-size: 16px;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .record-btn {
            background: #e74c3c;
            color: white;
        }
        
        .record-btn:hover {
            background: #c0392b;
        }
        
        .record-btn.recording {
            background: #27ae60;
            animation: pulse 1.5s infinite;
        }
        
        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }
        
        .switch-btn {
            background: #3498db;
            color: white;
        }
        
        .switch-btn:hover {
            background: #2980b9;
        }
        
        .status {
            text-align: center;
            padding: 15px;
            margin: 20px 0;
            border-radius: 8px;
            background: #ecf0f1;
            color: #2c3e50;
        }
        
        .conversation {
            max-height: 400px;
            overflow-y: auto;
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }
        
        .message {
            margin: 10px 0;
            padding: 10px;
            border-radius: 8px;
        }
        
        .user-message {
            background: #3498db;
            color: white;
            margin-left: 20%;
        }
        
        .ai-message {
            background: #ecf0f1;
            color: #2c3e50;
            margin-right: 20%;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¤ è¯­éŸ³é¢è¯•</h1>
            <p>ä½¿ç”¨è¯­éŸ³ä¸AIé¢è¯•å®˜è¿›è¡Œå®æ—¶å¯¹è¯</p>
        </div>
        
        <div class="voice-controls">
            <button id="recordBtn" class="voice-btn record-btn">å¼€å§‹å½•éŸ³</button>
            <button id="switchBtn" class="voice-btn switch-btn">åˆ‡æ¢åˆ°æ–‡æœ¬æ¨¡å¼</button>
        </div>
        
        <div id="status" class="status">
            å‡†å¤‡å°±ç»ªï¼Œç‚¹å‡»å¼€å§‹å½•éŸ³æŒ‰é’®å¼€å§‹è¯­éŸ³é¢è¯•
        </div>
        
        <div id="conversation" class="conversation">
            <div class="message ai-message">
                æ‚¨å¥½ï¼æˆ‘æ˜¯AIé¢è¯•å®˜ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨è¿›è¡Œè¯­éŸ³é¢è¯•ã€‚è¯·ç‚¹å‡»å½•éŸ³æŒ‰é’®å¼€å§‹æˆ‘ä»¬çš„å¯¹è¯ã€‚
            </div>
        </div>
    </div>
    
    <script>
        class VoiceInterview {
            constructor() {
                this.ws = null;
                this.isRecording = false;
                this.mediaRecorder = null;
                this.audioChunks = [];
                
                this.initElements();
                this.connectWebSocket();
            }
            
            initElements() {
                this.recordBtn = document.getElementById('recordBtn');
                this.switchBtn = document.getElementById('switchBtn');
                this.status = document.getElementById('status');
                this.conversation = document.getElementById('conversation');
                
                this.recordBtn.addEventListener('click', () => this.toggleRecording());
                this.switchBtn.addEventListener('click', () => this.switchToTextMode());
            }
            
            connectWebSocket() {
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                const wsUrl = `${protocol}//${window.location.host}/ws/voice-interview`;
                
                this.ws = new WebSocket(wsUrl);
                
                this.ws.onopen = () => {
                    this.updateStatus('WebSocketè¿æ¥å·²å»ºç«‹');
                };
                
                this.ws.onmessage = (event) => {
                    const data = JSON.parse(event.data);
                    this.handleWebSocketMessage(data);
                };
                
                this.ws.onclose = () => {
                    this.updateStatus('è¿æ¥å·²æ–­å¼€ï¼Œæ­£åœ¨é‡è¿...');
                    setTimeout(() => this.connectWebSocket(), 3000);
                };
                
                this.ws.onerror = (error) => {
                    this.updateStatus('è¿æ¥é”™è¯¯ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ');
                };
            }
            
            handleWebSocketMessage(data) {
                switch(data.type) {
                    case 'status':
                        this.updateStatus(data.message);
                        break;
                    case 'voice_response':
                        this.addMessage('user', data.transcription);
                        this.addMessage('ai', data.ai_response);
                        break;
                    case 'mode_switch':
                        if (data.new_mode === 'text') {
                            window.location.href = '/';
                        }
                        break;
                    case 'error':
                        this.updateStatus(`é”™è¯¯: ${data.message}`);
                        break;
                }
            }
            
            async toggleRecording() {
                if (!this.isRecording) {
                    await this.startRecording();
                } else {
                    this.stopRecording();
                }
            }
            
            async startRecording() {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    this.mediaRecorder = new MediaRecorder(stream);
                    this.audioChunks = [];
                    
                    this.mediaRecorder.ondataavailable = (event) => {
                        this.audioChunks.push(event.data);
                    };
                    
                    this.mediaRecorder.onstop = () => {
                        const audioBlob = new Blob(this.audioChunks, { type: 'audio/wav' });
                        this.sendAudioData(audioBlob);
                    };
                    
                    this.mediaRecorder.start();
                    this.isRecording = true;
                    this.recordBtn.textContent = 'åœæ­¢å½•éŸ³';
                    this.recordBtn.classList.add('recording');
                    this.updateStatus('æ­£åœ¨å½•éŸ³...');
                    
                    // å‘é€å¼€å§‹å½•éŸ³æ§åˆ¶æ¶ˆæ¯
                    this.sendControlMessage('start_recording');
                    
                } catch (error) {
                    this.updateStatus('æ— æ³•è®¿é—®éº¦å…‹é£ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®');
                }
            }
            
            stopRecording() {
                if (this.mediaRecorder && this.isRecording) {
                    this.mediaRecorder.stop();
                    this.isRecording = false;
                    this.recordBtn.textContent = 'å¼€å§‹å½•éŸ³';
                    this.recordBtn.classList.remove('recording');
                    this.updateStatus('å½•éŸ³å·²åœæ­¢ï¼Œæ­£åœ¨å¤„ç†...');
                    
                    // å‘é€åœæ­¢å½•éŸ³æ§åˆ¶æ¶ˆæ¯
                    this.sendControlMessage('stop_recording');
                }
            }
            
            sendAudioData(audioBlob) {
                const reader = new FileReader();
                reader.onload = () => {
                    const base64Audio = reader.result.split(',')[1];
                    this.ws.send(JSON.stringify({
                        type: 'voice_data',
                        audio: base64Audio
                    }));
                };
                reader.readAsDataURL(audioBlob);
            }
            
            sendControlMessage(command) {
                this.ws.send(JSON.stringify({
                    type: 'control',
                    command: command
                }));
            }
            
            switchToTextMode() {
                this.sendControlMessage('switch_to_text');
            }
            
            updateStatus(message) {
                this.status.textContent = message;
            }
            
            addMessage(sender, content) {
                const messageDiv = document.createElement('div');
                messageDiv.className = `message ${sender}-message`;
                messageDiv.textContent = content;
                this.conversation.appendChild(messageDiv);
                this.conversation.scrollTop = this.conversation.scrollHeight;
            }
        }
        
        // åˆå§‹åŒ–è¯­éŸ³é¢è¯•
        document.addEventListener('DOMContentLoaded', () => {
            new VoiceInterview();
        });
    </script>
</body>
</html>
        """
    
    def get_integration_status(self) -> Dict[str, Any]:
        """
        è·å–é›†æˆçŠ¶æ€
        
        Returns:
            é›†æˆçŠ¶æ€ä¿¡æ¯
        """
        return {
            "voice_chat_enabled": self.voice_stream is not None,
            "active_sessions": len(self.active_sessions),
            "session_types": [session["type"] for session in self.active_sessions.values()]
        }


# åˆ›å»ºå…¨å±€é›†æˆå®ä¾‹
voice_integration = VoiceInterviewIntegration(main_app)


def setup_voice_integration(api_key: Optional[str] = None, model_name: Optional[str] = None):
    """
    è®¾ç½®è¯­éŸ³é›†æˆåŠŸèƒ½
    
    Args:
        api_key: Google AI APIå¯†é’¥
        model_name: Geminiæ¨¡å‹åç§°
    """
    try:
        voice_integration.setup_voice_chat(api_key, model_name)
        voice_integration.mount_voice_endpoints()
        logger.info("è¯­éŸ³é›†æˆåŠŸèƒ½è®¾ç½®å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"è¯­éŸ³é›†æˆåŠŸèƒ½è®¾ç½®å¤±è´¥: {e}")
        return False


# æ·»åŠ åˆ°ä¸»åº”ç”¨çš„å¯åŠ¨äº‹ä»¶
@main_app.on_event("startup")
async def setup_voice_on_startup():
    """
    åº”ç”¨å¯åŠ¨æ—¶è‡ªåŠ¨è®¾ç½®è¯­éŸ³åŠŸèƒ½
    """
    try:
        # å°è¯•è‡ªåŠ¨è®¾ç½®è¯­éŸ³åŠŸèƒ½
        setup_voice_integration()
        logger.info("è¯­éŸ³åŠŸèƒ½è‡ªåŠ¨è®¾ç½®å®Œæˆ")
    except Exception as e:
        logger.warning(f"è¯­éŸ³åŠŸèƒ½è‡ªåŠ¨è®¾ç½®å¤±è´¥ï¼Œå°†åœ¨æ‰‹åŠ¨é…ç½®åå¯ç”¨: {e}")


# æ·»åŠ çŠ¶æ€æ£€æŸ¥ç«¯ç‚¹
@main_app.get("/api/voice/status")
async def get_voice_status():
    """
    è·å–è¯­éŸ³åŠŸèƒ½çŠ¶æ€
    """
    return voice_integration.get_integration_status() 